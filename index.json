[{"categories":null,"contents":"This post is an extension of the great tutorial \u0026ldquo;Hacking my Kobo Clara HD\u0026rdquo;, with the some extra details and clarifications from my experience.\n1. Installing KOReader This thread has the installation information: link. 1. Search for \u0026ldquo;One-Click Kobo Packages\u0026rdquo;, and download the desired package zip (I just did KOReader and Plato) 2. Search for your OS (\u0026ldquo;Windows\u0026rdquo;, \u0026ldquo;macOS\u0026rdquo;, or \u0026ldquo;Linux\u0026rdquo;) and download the \u0026ldquo;install script archive\u0026rdquo; and unzip it in the same folder as the package zip. 3. Run ./install.sh; follow any prompts 4. Safely eject the device, reboot, and profit! 2. Configuring SSH Plug in your device via USB and mount it (probably done automatically) Copy in your public RSA (non-RSA might not work, but did not confirm) SSH key into: 1 \u0026lt;Full Path to Reader\u0026gt;/KOBOeReader/.adds/koreader/settings/SSH/authorized_keys`. For reference: For reference:\n1 2 3 4 # My full path on Linux looks like: /run/media/horv/KOBOeReader/.adds/koreader/settings/SSH/authorized_keys # My copy command looks like: cp ~/.ssh/id_rsa.pub /run/media/horv/KOBOeReader/.adds/koreader/settings/SSH/authorized_keys Safely eject the device SSH into device following [KOReader docs](Connect to the e-reader via SSH) Transferring files via SSH: 1 scp -P 2222 \u0026lt;Local File Path\u0026gt; root@\u0026lt;Your Kobo IP\u0026gt;:\u0026lt;Destination Path on Kobo\u0026gt;` 3. Installing Syncthing Install latest ARM binary (32-bit for Kobo Clara HD) SSH into device (see step 2) Make Syncthing config directory: mkdir -p ~/.config/syncthing Add config: 1 2 3 4 5 echo \u0026#39;\u0026lt;configuration version=\u0026#34;18\u0026#34;\u0026gt; \u0026lt;gui enabled=\u0026#34;true\u0026#34; tls=\u0026#34;false\u0026#34; debugging=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;address\u0026gt;0.0.0.0:8384\u0026lt;/address\u0026gt; \u0026lt;/gui\u0026gt; \u0026lt;/configuration\u0026gt;\u0026#39; \u0026gt; ~/.config/syncthing/config.xml Copy a valid certificate authority (ca-certificates.crt) to device from your own. For me it was: 1 scp -P 2222 /etc/ssl/certs/ca-certificates.crt root@\u0026lt;Your Kobo IP\u0026gt;:/etc/ssl/certs/ Run syncthing over SSH: /mnt/onboard/.adds/syncthing\nOpen syncthing via a browser by going to: - [http://\u0026lt;Your Kobo IP\u0026gt;:8384/](http://:8384/) (assuming default syncthing port is set) For security, add a username and password to Syncthing Search \u0026ldquo;figure out how to start it\u0026rdquo; in \u0026ldquo;Hacking my Kobo Clara HD\u0026rdquo; for several methods on how to start Syncthing on the device ","date":"Mar 31","permalink":"https://h0rv.github.io/posts/hacking-kobo-clara-hd/","tags":["ereader","kobo","hacking","reading","offline"],"title":"Hacking my Kobo Clara HD in 204"},{"categories":null,"contents":" Last edited: March 31, 2024\nMy favorite books, podcasts, and other media sources.\nBooks (Unordered and incomplete)\nSci-fi\nNeuromancer by William Gibson Dune I \u0026amp;\u0026amp; II by Frank Herbert Dark Matter by Blake Crouch Hyperion by Dan Simmons Bloodchild by Octavia E. Butler Project Hail Mary by Andy Weir Non-fiction\nDigital Minimalism by Cal Newport Permanent Record by Edward Snowden The Innovators by Walter Isaacson Life 3.0 by Max Tegmark Fiction\nBrave New World by Aldous Huxley 1984 by George Orwell Fight Club by Chuck Palahnuik Podcasts Science and Technology\nLex Fridman Podcast Darknet Diaries The a16z Podcast Huberman Lab Artificial Intelligence\nLatent Space Dwarkesh Podcast Work and Career\nDeep Questions History\nDan Carlin\u0026rsquo;s Hardcore History YouTube Channels Technology\nTom Scott\nSoftware Engineering and Programming\nComputerphile ThePrimeagen No Boilerplate george hotz archive TJ DeVries CodeAesthetic Dreams of Code Artificial Intelligence\nAndrej Karpathy Two Minute Papers Jeremy Howard Privacy, Security, and Linux\nNBTV: Naomi Brockwell TV\nGNU/Linux, Open Source, and Free Software\nLuke Smith Mental Outlaw DistroTube Favorite of all: KRAZAM\nLifestyle\nDigital Minimalism\njvsholz and jay skullz Tiny home/living\nNEVER TOO SMALL Modern House Cabin People\nCasey Neistat Music\nOdysseus\nHouse Sets\nChris Luno Boiler Room Book Club Radio Blogs Technology Geohot\u0026rsquo;s Blog\nLuke Smith\u0026rsquo;s Blog\nMarc Andreesen\u0026rsquo;s Blog\nSam Altman\u0026rsquo;s Blog\nSchneier on Security\nArtificial Intelligence\nAI Engineering Latent Space Swyx\u0026rsquo;s Blog Alessio Fanelli Films Shows Music ","date":"Mar 31","permalink":"https://h0rv.github.io/mediashelf/","tags":["media"],"title":"Mediashelf"},{"categories":null,"contents":" ","date":"Mar 25","permalink":"https://h0rv.github.io/board/","tags":null,"title":"Mood Board"},{"categories":null,"contents":" Last edited: March 25, 2024\nHi, I am Robby! I am a software engineer and digital minimalist.\nI am passionate about open souce, infrastructure, and AI. Additionally, I care deeply about the effects that technologies have on society, in particular: addictive technologies, detrimental consquences of centralization, and AI.\nI don\u0026rsquo;t have any socials but you can contact me at:\n1 2 # Email echo cmhvcnZAcHJvdG9uLm1lCg== | base64 -d ","date":"Feb 27","permalink":"https://h0rv.github.io/about/","tags":null,"title":"About"},{"categories":null,"contents":" I wanted to share my final essay for a Philosophy of Science class focused on Artificial Intelligence from the Spring of my 2rd year in University - April 25, 2022 to be exact. This was my first and only Philosophy class I ever took. With that said, I really enjoyed the in-person discussions (that I was deprived of due to Covid-19 pandemic) and some of the readings. It was by far the most academic papers I had to read in a course (several weekly readings), which often resulted in having a strong urge to take a nap after 20 minutes of reading due to the density of the texts. I especially struggled with the metaphysical and abstract ideas becasue they really did not click for me which I just ended up getting boring. Nonetheless, it was one of my favorite courses I took in college, due greatly to the lecturer who had a great personality and was wickedly intelligent.\nI. Introduction Imperialism is defined as, according to the Merriam-Webster dictionary, \u0026ldquo;the policy, practice, or advocacy of extending the power and dominion of a nation especially by direct territorial acquisitions or by gaining indirect control over the political or economic life of other areas.\u0026rdquo; It took off when the world started to globalize in the 1800s and still prevails in certain parts of the world today. Imperialism is typically seen as negative due to it often changing and destroying cultures and values certain groups and societies held. This was, and still is, often done with ill intentions by the imperialists due to them taking a pompous stand over the individuals they exert their power over, which they believe their victims\u0026rsquo; values and culture are inferior to theirs. It is important to note that the advocacy of the imperialist\u0026rsquo;s policies does not make the imperialist\u0026rsquo;s values right and the victims of imperialism\u0026rsquo;s values wrong. The concept of imperialism, specifically value imperialism, is a main point of concern in the future of artificial intelligence and moral machines. Cave et al. explain the basis of value imperialism well: \u0026ldquo;the universalization of a set of values in a way that reflects the value system of one group (such as the programmers). This could be pursued intentionally or, perhaps more alarmingly, could also be perpetrated inadvertently if programmers unintentionally embed their values in an algorithm that comes to have widespread influence. Such value imperialism might affect, or disrupt, cultures differently, or degrade cultural autonomy.\u0026rdquo; Political and economical imperialism has been extremely destructive to cultures that impeded its progress. Artificial general intelligence (AGI) has the strong potential of becoming, unarguably, the most powerful and influential agent the human race has ever seen. Due to this anomaly, it has the probable cause for the changing of every society and culture into the one it sees fit and \u0026ldquo;right.\u0026rdquo; So, what is the \u0026ldquo;right\u0026rdquo; set of values for an AGI to possess? This is a nontrivial problem within itself and pertains to the machine ethics field of study. I am not going to address this problem in this paper because it is still largely unanswerable. I will, however, argue why the issue of value imperialism in AI can lead to various problems internationally, why it may even pose a threat to some nations, and will then offer plausible solutions to help mitigate these challenges. The idealization I will make for this argument is that an AGI is created in such a way that it coexists and cooperates with humans, rather than taking full control. In part two (II), I will talk about who, or what, actually has the ability to ingrain their values within an intelligent machine. In the third part (III), I will answer the question of why this concept of value imperialism is concerning due to the potentially drastic consequences that could come of it. In part four (IV), I will explore what path the current state of artificial intelligence research is currently on and where it is likely headed. Finally, in parts five (V) and six (VI), I will present potential solutions and paths that can be taken to help mitigate these dilemmas.\nII. Who has the ability to impose value imperialism? In the current state of artificial intelligence, it is not all clear who is going to be responsible for instilling values into the machine. There are several scenarios of who is going to be responsible for doing this. The first is the individual programmers. The programmer may be responsible for defining the morals and values in an intelligent machine, either purposely or unknowingly. The programmers are responsible for creating the algorithms or model, and as such are capable of embedding their own values and morals into the machine. This is quite dangerous because no individual, nor a small team of programmers, has the capacity of defining a set of values into a machine that is representative of all that the machine will influence, be in contact with, and overall, impact greatly. The next group that is able to imperialize the ethics within a machine is a single company. Companies typically have a set of values and goals to which it strives to hold themselves to and reach. Assuming a company truly believes in these values and makes decisions reflecting them and does not use them for mere public relations. So, a company has its programmers creating the AGI, so it is the proprietary property of the company. Therefore, the company would ideally oversee this project and ensure it represents the company\u0026rsquo;s values, rather than one of its employees. A company in itself is simply, according to a famous Supreme Court decision by Chief Justice John Marshall, \u0026ldquo;\u0026hellip;an artificial person, invisible, intangible, and existing only in contemplation of the law\u0026rdquo; (Pride et al.). With this being said, it shows that a company is susceptible to just the same consequences that can be caused by individual programmers, but here it would simply be done collectively by many individuals who represent the \u0026ldquo;artificial person.\u0026rdquo;\nLastly, governmental organizations have the ability to define the values and ethics within an intelligent machine. Modern governments represent and influence large masses of people, far more than any individual person or company is capable of. This then seems like the ideal agent to hold the responsibility of imposing its values into an AGI. Governments already impose policies and laws which their citizens are implied to follow. However, governments come in many forms: democratic, totalitarian, communist, etc. Due to these different forms of government being radically different, the values it would give to an AGI would likely be much different. As well as the fact governments, considering there does not exist a cosmocracy (world or global government), only represent a single nation. As we will see, values vary wildly between nations; depending on which government has the power and control of instilling the AGI with values, it can be quite problematic for another nation.\nIII. Why is this issue currently of concern? The US, Russia, and China are said to be in a Cold War/Arms Races with AI development, which is irresponsible and dangerous for all. But this brings ethics into the field, because Western values differ, sometimes dramatically, from the East. The former consists of protecting basic human rights, equality, free speech, and liberty. In the latter, these same rights that are assumed to be guaranteed to each of their citizens are often infringed upon. China believes censoring information that enters and leaves its country\u0026rsquo;s internet and restricting the media is beneficial to the country\u0026rsquo;s future. While in the West\u0026rsquo;s eyes, this is unethical and everyone should have the right to the free flow of information. Ideas of values and principles that are dissimilar to what currently exists in a specific society are extremely powerful. A quintessential example of this is the West\u0026rsquo;s, especially the United States of America\u0026rsquo;s, reaction to the Communist revolution following World War II. Communism debased nearly all the principles upon which the United States was founded on. This created immense tension between the Soviet Union and the United States, which technically did not lead to any direct conflicts but rather the support of proxy wars fought by third-party nations. Regardless, there was a battle between values, principles, and generally, information. This shows how threatening creating an AGI, which could, with high probability, be the most powerful agent the world could see up to that point. It would be greatly influential and no nations would likely be able to even compete with it in an information war of the same type as the Cold War. So, if this agent has opposing views of one or a group of nations, it would be quite threatening to them and immediate action would likely need to be taken. Neuroscientist and philosopher Sam Harris puts this into perspective: \u0026ldquo;And what would the Russians or the Chinese do if they heard that some company in Silicon Valley was about to deploy a superintelligent AI? This machine would be capable of waging war, whether terrestrial or cyber, with unprecedented power. This is a winner-take-all scenario. To be six months ahead of the competition here is to be 500,000 years ahead, at a minimum. So it seems that even mere rumors of this kind of breakthrough could cause our species to go berserk\u0026rdquo; (9:22). Due to the immense power an artificial intelligence would grant its users or owners, it does not make it a far stretch to say that the technological feat of achieving a \u0026ldquo;true\u0026rdquo; artificial general intelligence is a war-starting event, of course only if development and research stay on the same path it is currently on.\nIV. So what is the track we are currently on? Artificial intelligence\u0026rsquo;s current development and research are currently geared towards an economic reward in the industry at companies like Facebook and Microsoft or as a weapon in a world where cyberwar is becoming all the more relevant. Regardless of its use case, both approaches seem to incentivize fast progress without being precautionary. Capitalism, in general, incentivizes progress to gain an economic edge in an industry, while being more reactionary, rather than proactive, to consequences. For example, look at product and drug recalls in the United States. Products are put to use in the real world and policies and laws are often put in place after any consequences are seen. This is rightly so because one can only predict so much that can happen in the real world with so many different acting variables. The atomic bombs use is another perfect example of AI development as well. Technological advancements excelled dramatically during World War II and resulted in the creation of the atomic bomb by Allied powers, and its actual use against Axis powers. Then only after its use was rational policies put in place to control nuclear weapon use both nationally and internationally. The high-stakes environment World War II produced made the creation possible much quicker than anticipated. The main point I am trying to make here is that an artificial intelligence arms race is creating an environment very similar to one seen in World War II. This is hazardous due to the rushed use of the products designed and created in said environments. An AGI is likely not \u0026ldquo;recallable\u0026rdquo; and would have an impact touching every part of the world, not restrained to just two cities like the atomic bomb. So, being both persistent and not restricted to a certain and small environment, makes a dire combination for artificial intelligence. Then, there is a possibility that the release of an AGI does not lead to a war or stringent responses from different nations. Although I believe this is unlikely, it is of course imaginable. In this reality, due to an AI\u0026rsquo;s overreaching influence, the world, as a whole, would likely converge to the values and principles implemented in it. This would likely result in a cultural genocide globally and is certainly undesirable. On the other hand, a world with the same values and principles would be one with far less conflict and national tension, but it is strongly dependent on what these values and principles actually are: democratic or totalitarian.\nV. Potential Solution: Diplomacy The first potential solution to alleviate these potential destructive consequences that AGI can produce is international diplomacy. Global regulations and overseeing of artificial intelligence development can help control and monitor the transition to an AGI inhabited world. This seems like the ideal situation in which all nations, and thus most widespread representation, have a say in the values and principles in which intelligent machines should pursue. However, this, unfortunately, also seems least likely due to diplomacy often being easier said than done. Not to mention, the participation in this diplomacy would likely be voluntary; so nations can just outright not participate. Regardless, international efforts should still be enacted. Getting the sobering reality of the future of AI into the mainstream discourse is important for all due to the great changes it will have on the future of mankind at large. Companies and institutions leading the world in cutting-edge AI development and research should also deem responsibly and put efforts into ethical research for AI. Thankfully, this is already happening at many universities and companies, but there will likely never be a sufficient amount, and the more to put the effort in, the better. The general goal is an international discourse on the hypothetical dangers and concerns revolving around AI: only one specifically being the circumventing the installation of value imperialism within an AI. It is important for researchers, scientists, philosophers, and programmers to demystify AI at large and make plain what the technology can offer, yet also take away.\nVI. Potential Solution: AI decides its own values This problem of value imperialism, in general, is that it makes a significant assumption about the fundamental essence of an artificial general intelligence: its values are embedded by humans—its creators. There is a possibility that the AI, in turn, would be able to generate its own set of values and principles that do not align directly with any one nation or group of people. It will be a superintelligence, after all, so why would it not be able to create a set of values better than humans have currently achieved? This would naturally render the problem of value imperialism, and all its consequences, void. Nonetheless, the nature of AI\u0026rsquo;s value system is still a largely unanswerable question and may only be answerable when a \u0026ldquo;true\u0026rdquo; AI is created.\nVII. Conclusion Additionally, due to the grave dangers that the current approach and development of AI poses, it is absolutely necessary for international discourse, research, and diplomacy surrounding not just the ethics, but many aspects of AI to be put into stride. Like most diplomacy, it will likely be a lengthy and hard process, but is of utmost importance; so immediate attention is definitely favorable, rather than waiting until it\u0026rsquo;s too late. Sam Harris articulates the general current state of mind of AI: \u0026ldquo;Now, one of the most frightening things, in my view, at this moment, are the kinds of things that AI researchers say when they want to be reassuring. And the most common reason we\u0026rsquo;re told not to worry is time. This is all a long way off, don\u0026rsquo;t you know. This is probably 50 or 100 years away. One researcher has said, \u0026lsquo;Worrying about AI safety is like worrying about overpopulation on Mars\u0026rsquo;\u0026rdquo; (9:54). If this reassurance is used over and over, it will be too late for any essential progress to be made before a superintelligence enters our world. The many goals, only a single one including the prevention and general avoidance of value imperialism as talked about in this paper, that need to be achieved before the deployment of an artificial general intelligence is no easy task if a full effort was put forth today, but if this is suspended to the late future, there is a good chance the technology runs away while the slow-moving political processes governing artificial intelligence lags behind with no way of catching up.\nWorks Cited Cave, Stephen, et al. “Motivations and Risks of Machine Ethics.” Proceedings of the IEEE, vol. 107, no. 3, 2019, pp. 562–574., doi:10.1109/jproc.2018.2865996. Harris, Sam. “Can We Build AI without Losing Control over It?” Sam Harris: Can We Build AI without Losing Control over It? | TED Talk, \u0026lt;www.ted.com/talks/sam_harris_can_we_build_ai_without_losing_control_over_it/transcript\u0026gt;. “Imperialism Definition \u0026amp; Meaning.” Merriam-Webster, Merriam-Webster, www.merriam-webster.com/dictionary/imperialism. Pride, William M., et al. Business. Houghton Mifflin Co., 1996. ","date":"Oct 27","permalink":"https://h0rv.github.io/posts/value-imperialism-in-ai/","tags":["technology","ai"],"title":"Value Imperialism in the Future of Artificial Intelligence"},{"categories":null,"contents":"\nThe announcement of the Apple Vision Pro yesterday left me very conflicted. On one hand, the productivity gains that can be yielded by replacing the standard desk and monitor setups for myself and many others (and being able to easily travel with such large \u0026ldquo;screen space\u0026rdquo;) is very exciting. On the other, the dystopic future that was once predicted in 80\u0026rsquo;s science fiction is one step closer to becoming realized.\nSpatial computing does seem to be the future and has been a long time coming with virtual reality headsets, but now it seems all the technologies, both software, and hardware, are falling together to make such devices now possible. The current limitations of screens, especially on mobile computing, certainly limits the way we can interact with our devices. For example, I choose to use a tiling window manager to maximize screen size on my laptop. Is this the best way to interface with my computer? For a small screen space, I think yes, but in general? Likely not. This technology opens many doors to the future of computing and interfaces with the technologies we all use daily.\nBut I suppose this greatly depends on how one uses such technologies daily. If one interacts mostly with their small-sized smartphone, exploring the cesspool of what are the endless feeds of social media and entertainment apps like my current least favorite thing in the world, TikTok. The current use of these technologies, from my use of experiences and observations of others, is opening an app Instagram for a few swipes, swiping to Twitter, opening a Snapchat notification, responding to a friend or group chat, opening TikTok, getting bored, and repeat the cycle back on Instagram (or another most desired app). But great news, this wasted time flipping through apps can come to an end! With such new technologies, one can have a YouTube video playing, while having their Snapchat open, flipping through their friends\u0026rsquo; stories, texting in an iMessage group chat, all while scrolling for new crap they don\u0026rsquo;t need on Amazon. Doesn\u0026rsquo;t that sound wonderful?\nPart of this truly scares me. I witness individuals scroll on TikTok, with their audio muted for extended periods. So, if an app like TikTok can be so addicting and stimulating on a small, 6-inch screen with, as seen, without audio not even necessary, how high is the ceiling of entrancement such apps can cast on their users with the levels of immersion spatial audio and video can give?\nThat is why I was always fascinated with the term \u0026ldquo;wirehead,\u0026rdquo; introduced to me by William Gibson in his Sprawl Trilogy. He used it to refer to individuals constantly \u0026ldquo;jacked-in\u0026rdquo; into cyberspace because whatever was in there, was \u0026ldquo;better\u0026rdquo;, at least in the current moment more entertaining or stimulating, than the life, their life, right in front of them. I always thought it was funny due to how closely this resembles the masses\u0026rsquo; current use of smartphones, but now the actual wirehead is near.\n","date":"Jun 06","permalink":"https://h0rv.github.io/posts/wireheads-are-coming/","tags":["digital minimalism","technology","apple vision","spatial computing","dystopia"],"title":"Wireheads are Coming"},{"categories":null,"contents":"To Zuck,\nHumans should not have access to virtually infinite streams of information. Likewise, humans also should not smoke cigarettes or alcohol, but they have an individual choice (as they should) to make an educated decision to do so or not. One difference, however, is that the latter products come with warning labels, while the former do not. This is of great concern to me, due to the impact it had on me and the imagined impact I can see it having on the younger generation currently, when all the things I talk about are only amplified to the extreme.\nDownloading Instagram when I was in fifth grade seemed like a harmless act that everyone else was doing when it was new and exciting. The only thing the app confirmed was if you were over the age of 13 (for data privacy laws), which as a clever 10-year-old, clicking the checkbox stated \u0026ldquo;Yes\u0026rdquo; was not too hard to bypass. And with this sign-up, came a nearly decade-long drought of reading, and other forms of high-quality leisure activity, with an irreplaceable, yet much easier, the alternative of mindless scrolling and consuming low-quality forms of media. During my extended usage of this app, and I would shamelessly say it was likely way over-usage, I came to a realization in my early undergraduate years that I had a problem. This was the hardest and most consistently I was challenged academically, and as a result of this, I had to learn to actually study and focus. Unsurprisingly, I struggled to focus immensely and was hit by this when I received a C or D on my first exam for multi-variable calculus in my first semester. I always excelled at Math my whole life and never had to do any supplementary work to pass with 100s. Admittedly, thinking college was going to be the same way was dumb, nonetheless, I had to start studying. Long story short, I struggled to sit down and study and work deeply with my phone at the back of my mind at all times. This is when I started to question my relationship with social media apps, and technology at large, which finally led up to, ending my second year of college, fully deleting all of my social media accounts, including Instagram and Facebook. Instagram was my app of choice and could scroll for hours on the app. I sound like the perfect \u0026ldquo;user\u0026rdquo; of the app for your monetization model of the app: collecting large amounts of ad revenue. But at what cost? Not only was my ability to focus completely massacred from unhealthy usage of the app, but nearly 10 years of doing (literally) anything else was robbed of me. I started reading more after abandoning social media. Due to this, being robbed of 10 years worth of books where I could have gained vast amounts of knowledge or read something that had a profound impact at such a developmental stage of my life, is saddening. Instead, I consumed low-quality media in mass abundance, and looking back, not a single one had even the slightest impact on my life. In fact, there is not a particular post that I can even recall in general, or let alone say that it made me a better person in the slightest way.\nAll this came with no warning labels. And, from my knowledge, it still doesn\u0026rsquo;t. So, on your products at Meta, I think it would be in everyone\u0026rsquo;s best interest if you would include warnings of the (high) potential things individuals will sacrifice by installing your apps.\nGo Fuck Yourself,\nRobby\nBackground This was for an assignment for my \u0026ldquo;professional\u0026rdquo; writing course, and, specifically, we had to do a complaint letter. I was happy with the outcome and thought I would share.\n","date":"Jan 26","permalink":"https://h0rv.github.io/posts/a-letter-to-zuck/","tags":["digital minimalism","social media","technology"],"title":"A Letter to Zuck"}]